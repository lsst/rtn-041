\documentclass[OPS,authoryear,toc]{lsstdoc}
\input{meta}

% Package imports go here.

% Local commands go here.

\title{Data Preview 0.2 and Operations rehearsal for DRP.}

\author{%
  William O'Mullane,
  Fritz Mueller
}

\setDocRef{RTN-041}
\setDocUpstreamLocation{\url{https://github.com/lsst/rtn-041}}

\date{\vcsDate}

\setDocAbstract{%
DM delivered software to operations to perform processing of the DESC DC2 data as well as enhancements to the
portal and Qserv for interaction with the results.  The release of this was called Data Preview 0.2 and the
production of the data products and publication of them were carried out in an operational manner. This
provides valuable insights for operational data releases.
}

% Change history defined here.
% Order: oldest first.
% Fields: VERSION, DATE, DESCRIPTION, OWNER NAME.
% See LPM-51 for version number policy.
\setDocChangeRecord{%
  \addtohist{1}{2022-08-03}{Unreleased.}{William O'Mullane}
  \addtohist{1}{2022-09-08}{Added Qserv section.}{Fritz Mueller}
}

\begin{document}

% Create the title page.
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Qserv}\label{sec:qserv}

Qserv is a horizontally scalable parallel SQL database system, developed by the Rubin Observatory specifically
to support the LSST production catalog use case.  Qserv is further described in \citeds{LDM-135}.

The DP0.2 simulated survey covers about 1/60th of the anticipated on-sky area of the full survey (roughly
300/18,000 sq. degrees), at about half the anticipated observational depth of the full survey.  The generated
catalogs contain \textasciitilde{}136 billion rows in total.  On disk, including necessary indexes, this
amounts to \textasciitilde{}30 TiB of database storage.  A Qserv cluster with one head node and five worker
nodes was comissioned in the IDF cloud computing environment to host and serve this data.

\subsection{Enhancements delivered during DP0.2}

\begin{itemize}

\item
  \textbf{Ingest systems:} At these scales, a trouble-free database ingest from DRP pipeline outputs to a
  complete online database takes several days.  All catalog data must be format converted from the parquet
  outputs as delivered by DRP to CSV files needed for efficient load at the Qserv worker nodes.  Data after
  conversion must be sharded, distributed, loaded at worker nodes, and appropriate indices compiled.  A
  significant fraction of the Qserv codebase is concerned only with the management, tracking, and automation
  of these ingest activities.

  Enhancement of these Qserv subsystems continued during the course of the DP0.2 exercise, and improvements
  were deployed continuously into production.  These improvments were aimed at streamlining the ongoing ingest
  campaigns, and included such features as provision of fully asynchronous task APIs to upstream ingest
  tooling, refinements to the ingest state machine for improved observability of ongoing ingest activities,
  and finer-grained control over publication/retraction/replacement life-cycle of individual tables.

\item
  \textbf{Diagnostic monitoring:} Qserv provides its own in-built administration dashboard, which has proved
  to be an indispensible tool for monitoring and troublshooting operating instances.  This subsystem also saw
  lots of improvement during the course of DP0.2, including instrumentation of many additional aspects of
  Qserv related to ingest and query processing activities, an enhanced query monitor / query history, and
  overall performance improvments to the administration dashboard itself.

\item
  \textbf{Optimized point-in-polygon spherical geometric queries:} Rather late in timeline for DP0.2 a
  decision was taken to host IVOA ObsCore observational metadata within Qserv, since doing so would enable a
  much richer interaction model with image data and image services being offered for the first time in DP0.2.
  This interaction model required support for point-in-polygon spherical geometry queries, and needed spatial
  indexing features beyond the previously established scope of Qserv for DP0.2 in order to perform acceptably.
  A workable strategy for efficiently processing these sorts of queries based on integration of several bits
  of existing functionality was developed and successfully delivered into production.

\item
  \textbf{Photometry UDFs:} Some photometry-related UDFs were developed and contributed upstream to the SciSQL
  project (https://github.com/smonkewitz/scisql), along with general modernization and infrastructural
  improvements of SciSQL.  These UDFs can simplify construction of queries against the LSST data model, which
  to date is committed to provision of linear flux measurements rather than logarithmic magnitudes
  (\citeds{Document-27758}).

\end{itemize}

\subsection{Some lessons learned}

DP0.2 proved to be a very valuable exercise in shaking down Qserv and surrounding tooling and operating
processes.  Some take-aways related providing catalog database service for DP0.2:

\begin{itemize}

\item
  \textbf{Timely schema metadata is hard to get.}  Rubin's middleware has an extremely flexible data
  model.  While this provides a lot of agility in pipeline development, when it comes to provision of
  catalog services at some point before database ingestion a concrete schema commitment must be made.

  While Rubin does have an accepted format for such schema descriptions and a sufficient source control and
  deployment practice around this, the schema descriptions themselves run to the many thousands of lines and
  nobody chomps at the bit to make sure these are maintained, current, complete, and error free.  Otherwise
  ready-to-go catalogs have on more than one occasion been held up in release by the need for last-minute
  revisions to the schema descriptions.

  Remediation: much has been done already to drive schema consumers toward a single source of truth and to
  streamline the deployment and update of schemas, once updates are committed to source control.  Since
  schema description deliverables have consistently been seen to lag, it remains for management to
  appropriately pre-load the demand for these.

\item
  \textbf{Data curation -- try (even) harder.}  Often times, catalog data curation issues are not easily
  detected until the "end of the line", when a completed data release is presented for database ingestion.
  Representation issues, key constraints, and referential integrity issues may not be apparent until a full
  set of tables is available.  Some curation issues may not even be detected until \emph{after} database
  ingestion, when the catalogs can first conveniently be queried in the large.

  For DP0.2, there was a plan to mitigate turbulence at the end of the line by obtainin and ingesting a
  representationally complete subset of catalog products early in the release cycle.  In practice, this had
  only limited success.  Inevitable delays on all fronts compressed the time between availability of the
  subset products and the full release products.  Additionally, sequencing of DRP activity meant while some
  subset tables were available usefully early in the release cycle, others were not available until much
  later, closer to the full release.  Predictably, curation issues \emph{did} exist that thus were not
  identified until fairly late in the release cycle, and release of several tables in the data products was
  delayed while these tables were re-processed upstream and then re-ingested into the database.

  Remediation: work with DRP to see if a more complete set of tables can feasibly be obtained earlier in the
  release cycle. Schedule subset production and ingest even more conservatively.

\item
  \textbf{Watch out for automated test lacunae.}  Qserv contains some distributed reference match machinery to
  facilitate performant joins between catalogs.  It turns out that DP0.2 includes a truth table and associated
  match table which depend upon this functionality.  Though this feature is not otherwise commonly in use, the
  Qserv development team maintained a high level of confidence that it was ready to go, because it had known
  coverage in the automated integration test suites, and "if it was broken, we'd know about it."

  As it turned out, the relevant integration tests had been commented out of the test suite sometime in the
  previous year as a temporary measure while working through some containerization/deployment issues, test
  coverage had never been subsequently restored, and the uncovered feature, when put into use, was found to
  have some minor issues.

  Remediation: a thorough survey of the test suites was conducted, and commented/disabled tests were updated
  as necessary and returned to active service.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{References}\label{sec:bib}
\renewcommand{\refname}{} % Suppress default Bibliography section
\bibliography{local,lsst,lsst-dm,refs_ads,refs,books}

\section{Acronyms}\label{sec:acronyms}
\input{acronyms.tex}

\end{document}
